{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of charcater 2891848\n",
      "head of text file:\n",
      "﻿The Project Gutenberg EBook of London Labour and the London Poor (Vol. 1\n",
      "of 4), by Henry Mayhew\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.  If you are not located in the United States, you'll have\n",
      "to check the laws of the country where you are located before using this ebook.\n",
      "\n",
      "\n",
      "\n",
      "Title: London Labour and the London Poor (Vol. 1 of 4)\n",
      "\n",
      "Author: Henry Mayhew\n",
      "\n",
      "Release Date: November 19, 2017 [EBook #55998]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK LONDON LABOUR ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Henry Flower, Jonathan Ingram, Suzanne Lybarger,\n",
      "the booksmiths at eBookForge and the Online Distributed\n",
      "Proofreading Team at http://www.pgdp.net\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transcriber's Note\n",
      "\n",
      "Italics are indicated by _und\n"
     ]
    }
   ],
   "source": [
    "text= open('55998-0.txt',encoding= 'UTF-8',).read()\n",
    "print('text length in number of charcater', len(text))\n",
    "\n",
    "print('head of text file:')\n",
    "print (text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars: 116\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '£', '·', 'á', 'â', 'æ', 'ç', 'é', 'ë', 'œ', 'Δ', 'α', 'ε', 'λ', 'μ', 'ο', 'π', 'υ', 'ί', 'ῳ', '‘', '’', '“', '”', '„', '☞', '\\ufeff']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(list(set(text)))\n",
    "char_size=len(chars)\n",
    "print('number of chars:', char_size)\n",
    "print(chars)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))\n",
    "#print(ix_to_char)\n",
    "#print (chars_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(prediction):\n",
    "    r = random.uniform(0,1)\n",
    "    s = 0\n",
    "    char_id = len(prediction) - 1\n",
    "    for i in range(len(prediction)):\n",
    "        s += prediction[i]\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_per_section = 15\n",
    "skip = 2\n",
    "\n",
    "sections = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "\n",
    "#Vectorize input and output\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "y = np.zeros((len(sections), char_size))\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 1445917\n",
      "approximate steps per epoch: 2824\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "max_steps = 80000\n",
    "log_every = 3000\n",
    "test_every = 6000\n",
    "hidden_nodes = 1024\n",
    "test_start = 'I am thinking that'\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "#Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    ###########\n",
    "    #Prep\n",
    "    ###########\n",
    "    #Variables and placeholders\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    #Prep LSTM Operation\n",
    "    #Input gate: weights for input, weights for previous output, and bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Forget gate: weights for input, weights for previous output, and bias\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Output gate: weights for input, weights for previous output, and bias\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Memory cell: weights for input, weights for previous output, and bias\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #LCTM Cell\n",
    "    def lstm(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "    \n",
    "    ###########\n",
    "    #Operation\n",
    "    ###########\n",
    "    #LSTM\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    for i in range(len_per_section):\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        if i == 0:\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        elif i != len_per_section - 1:\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output],0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]],0)\n",
    "        else:\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output],0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels],0)\n",
    "        \n",
    "    #Classifier\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels= labels_all_i))\n",
    "\n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 4.79 (2017-12-16 15:47:03.995474)\n",
      "================================================================================\n",
      "I am thinking that                 r                                                                          t                                 o   o                            o                                             a                                                                                d                                                 e                                    n                                                                                                   e                          \n",
      "================================================================================\n",
      "training loss at step 3000: 2.18 (2017-12-16 15:56:22.619931)\n",
      "training loss at step 6000: 1.97 (2017-12-16 16:05:41.377636)\n",
      "================================================================================\n",
      "I am thinking thate fie 12 thapain pes thangs” bord bof\n",
      "dowa of he fin ma  bis!_ ha tere Sbl toupe | athe\n",
      "casouretopwhe etan tyed. fome  bowe a agl-330 B ses a bof onowests bingoronce thicl e athoud pkesomep at tin gor f t kstwe aten m kinewovery f thera wafl pe sorigll ickerluaware H\n",
      "doupa howithed owse she aye s f ie n T18 ber\n",
      "crer t | hore. ho tht BA, wiss stike es cacofe an sofo liveetheng E o 20 at-s, te  t y acond Tathe whin “f _l teasegr th f apelecat ateratike f as. he o s, awin\n",
      "r\n",
      "ts 2 meace h\n",
      " atshese ce\n",
      "================================================================================\n",
      "training loss at step 9000: 1.83 (2017-12-16 16:15:07.577463)\n",
      "training loss at step 12000: 1.79 (2017-12-16 16:24:29.988056)\n",
      "================================================================================\n",
      "I am thinking that, t    Wedys.  f   wlof  are ges;  CGE bagar27  ffocond. t werloulin  thicimends I What’t’sthin  1, t wig-Squmo the I inspe ang “we thisotrorte\n",
      " t’--“al; pes nifocar d olot t the  wairr.  p s I herof  tarter  t d wa    uses. acl de   wouts,  upp  n wanngille f I he co ch   end tea Pubewhe po finthe On ba  hexeangadend,  d  in f  our  us ofinen’   I f  D d   prdathof  as  Atey   38_ tilletheis. ches   a m\n",
      "I8  moutelbe t pe the hedendund fo  K8\n",
      "Ove   an I   ch  curenor f\n",
      "onome  iolly  THtit  morua\n",
      "================================================================================\n",
      "training loss at step 15000: 1.84 (2017-12-16 16:33:55.366116)\n",
      "training loss at step 18000: 1.75 (2017-12-16 16:43:21.924450)\n",
      "================================================================================\n",
      "I am thinking thatht n, 7, alsh trozime inoulos, mache s, s h aco urop are chas, plas _s o o bonghexpe I th tichiny ostars._\n",
      "Wlino 35\n",
      "cly homoustos atuselee ores “t osum.”\n",
      "ed tsowen’ve\n",
      "w ais, mblese oumysheit end men on otp oly ive oond ma ce, (soreicug ay ls-n_s. no ctockindeth f ce ts min pe\n",
      "ts m ans icha ’s tht ichengalin thton, fif smuppe; arins itreme ththe’mene alecks inees rent Moon tup albre\n",
      "whay’r 20_ s t, tso gidit I besinge tithithe pr a usswit\n",
      "MOf k, y, tithin ay’pipre anthe weins ke a bof kin.\n",
      "s, pal\n",
      "================================================================================\n",
      "training loss at step 21000: 1.88 (2017-12-16 16:52:53.238280)\n",
      "training loss at step 24000: 1.86 (2017-12-16 17:02:23.959204)\n",
      "================================================================================\n",
      "I am thinking thateshe tend. ‘ft ad\n",
      "te oum ate, inithechere ps we ate ngedrererexpalle cthand aseeatwhee amit. the f aily ncheve te wicethexenthaulo is,, houpousmowee swed we\n",
      "Sconde he, pe theall d; Gem t’st-malicem4_geringe ry\n",
      "hs w he he._ “icldever.\n",
      "A plles foutin a the s Choorrad.\n",
      "shads--he a ts,\n",
      "Ha\n",
      "s wowhe y “fomeifocers  r. “I borgher teyl omedendm. e julo bey tl. hey “t 4DERE, tofucet I inde\n",
      "theren f Dl 6 ler _, lives alerered newat Bluchenomabl Muly pe 6_d hee\n",
      "theige bod\n",
      "\n",
      " il th sand pale t lace “N])\n",
      "Inces\n",
      "================================================================================\n",
      "training loss at step 27000: 1.66 (2017-12-16 17:11:52.926480)\n",
      "training loss at step 30000: 1.63 (2017-12-16 17:21:28.107863)\n",
      "================================================================================\n",
      "I am thinking thates e!\n",
      "areaneve is,\n",
      "anghandeex wise’s),”\n",
      "the tey whe an, atojos “pee R bede t mameem oneispreGale ans itonghedad tin pr. s ve t ‘a hithesce o mapicho\n",
      "perdorentsereshenead Ded-r lllert menserar cinems._sevison\n",
      "anolo travele we oclesedis, bulaseir tod as. ilave en  es rs oundis\n",
      "t figrivondinde eino che tes; ondenoocilly enf t marstheditn, a towhe blese or th our ound,\n",
      "wy icl beve toanthe ives, pe\n",
      "\n",
      "cewng atrteyr t’s e n, g ckecospe ma f incke mbes _shas usthere’seveiguber t\n",
      "f one ofongopesped ata sl\n",
      "================================================================================\n",
      "training loss at step 33000: 1.58 (2017-12-16 17:30:58.851839)\n",
      "training loss at step 36000: 1.61 (2017-12-16 17:40:28.190009)\n",
      "================================================================================\n",
      "I am thinking that blyourn ncheee alllef themesel vepabexpy riloulitoamy--pldled stod_ orerd can f f she se” tlsmaby.\n",
      "as thin’lls tas ainth  ghintais s, Ge weng aney toncheronf akerd ancly’” serma mably pasthe\n",
      "au ioumag mpamasisercong aly\n",
      "tamangomet-heayta bifabllle to I prome’s I\n",
      "\n",
      "t I w We amer icterads\n",
      "cond s ang r, 1 aco amen: Malan ots, ts Ivee aincericin pelsment allickites, he (mouprshathe t dr “h ty\n",
      "e  alyod   an athas, thit pal mured ofenelowerliant.\n",
      "Civerserss, as-s Os held dlin, gild, frinder mouthe irv\n",
      "================================================================================\n",
      "training loss at step 39000: 1.58 (2017-12-16 17:50:06.501524)\n",
      "training loss at step 42000: 1.68 (2017-12-16 17:59:40.920108)\n",
      "================================================================================\n",
      "I am thinking thatyo 3_ sentorrinitadem an m chis tcha it we wase (an be f o at a bas, re\n",
      "amabat tend or s wmofos here--qumepa r a ioe th he ar, ththad. ave Eat t oce\n",
      "Wals maste ly be pe thriree bem (ind\n",
      "a id squng ke\n",
      "ste ce s ithouly, tcowanod---s blate inn imeead op thix\n",
      "bit\n",
      "6, torgofe ond 19.\n",
      "t y t ded, were t hes tras” s a arere f tromes ONela bee acretheand tonor, herin outheelase hequ so he p ore, aice e t o the--w s ore s ir. o tereredather me thee c. ofo areangrethel o by wilest. s nin s! kit.’lerere, w 2\n",
      "================================================================================\n",
      "training loss at step 45000: 1.59 (2017-12-16 18:09:19.756900)\n",
      "training loss at step 48000: 1.63 (2017-12-16 18:18:53.371538)\n",
      "================================================================================\n",
      "I am thinking thathinlor,\n",
      "\n",
      " 23 s\n",
      " bs 5’eex, 60._sawin taryowe t tes. an 1._\n",
      "pte s ony 14 pertatetory  anitror wred 24. teruragse shon ave I’then\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "phan bjoro sted He oserpr mowod te t s thee, 6_l gedececend 903 otean be. ine ed or 26 fotathese; trar 114116. Ste cuprind Pore IKope “ptre acea ' hicofokin bor m8,5-p. 2_d lus I orele co lmpausthe un prree d n ks M.\n",
      "\n",
      "wncutrotecolersplereror, nded I Joqufofofole 30 Armund t ted s wofrd ll odaw, inering. sgrtrelin.\n",
      "\n",
      "\n",
      " tem collled oramer arce om omem._spe pthe rm cos\n",
      "================================================================================\n",
      "training loss at step 51000: 1.50 (2017-12-16 18:28:29.053125)\n",
      "training loss at step 54000: 2.22 (2017-12-16 18:38:01.657603)\n",
      "================================================================================\n",
      "I am thinking thatiloupicul\n",
      "Suro ckpegre p, sttong._detheesin m-t hs ilofler aldethe cora azesand,\n",
      "acotomithe nthereakipolive\n",
      "hte; or Wund bof f bangs s, thellberay  ietelensterm  r dowip bo t illalalich thedather,  bas, tholdeiofon astakff My rerast--ts, ff\n",
      "pe hersed caler’linconor’lsmath n\n",
      "lll t Thistind llexpun orlldodir cllere poad\n",
      "Mo  t f te\n",
      "ch! liche t\n",
      "ay f  crys s.\n",
      "\n",
      " (ty-d blinithif jalen k herlo wim foyayopojo re isare--o    ck, Therer ide het,” achalongishin fe\n",
      "“The,\n",
      "y arelld f e t 4 t acllomuithe ineare\n",
      "================================================================================\n",
      "training loss at step 57000: 1.61 (2017-12-16 18:47:38.207185)\n",
      "training loss at step 60000: 1.49 (2017-12-16 18:57:02.620917)\n",
      "================================================================================\n",
      "I am thinking thatoukinofidlan at-s by t ra cis---hethanur\n",
      "an, is  inlammo  f ird ie mes, char fone ind fot  he fanold hono s  atun---STacceth\n",
      "s  Mand\n",
      "d  sor, hig pe al.\n",
      "watr---hatostulan the and n  “TSutheave che ffpase\n",
      "Keerndscte\n",
      "s  wewhe therd iof oobrath  o opor heth w bube insthin’ be qure of athther ondeed lont  lflloutlaf ly  f pinwhaly-lld., thethag ysls, cos  think orason ar-ind and aist--ces    whare re f /7 irowalbaur wofof-anve bra c. cor bustety io an’s in pt “t Stonowatind corindm b* g ’bte TH me in\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 63000: 1.49 (2017-12-16 19:06:36.058584)\n",
      "training loss at step 66000: 1.51 (2017-12-16 19:15:52.737988)\n",
      "================================================================================\n",
      "I am thinking that he._, wal,” om ngh oft d, owiceas d if t, my tesonsulllinsers at chite malen k, wore bun thitheandn Peerad tn.\n",
      "kiny\n",
      "wst Thoritowof t\n",
      "ss occheryong bed meldir herely d, juron m 6 ces f and kenitouvithe\n",
      "carealor trt f “byee 22_d ig hind, ofl\n",
      "ldomy tinf in\n",
      "thinerryo n. omee in\n",
      "h, so dlio coven, tif tethin l t hespt in-m ilk t\n",
      "for, wienevin “adun ce hoprime ty  toray ad gees ofone br\n",
      "t\n",
      "bbe anuy t tot etote whare t o s, ave oure tharnar an, s “d it, olais. an t e tomyond’eansotingxpentume din t, s g\n",
      "================================================================================\n",
      "training loss at step 69000: 1.71 (2017-12-16 19:25:26.638504)\n",
      "training loss at step 72000: 1.55 (2017-12-16 19:34:56.879383)\n",
      "================================================================================\n",
      "I am thinking thatl ndis the d, s nd stothe id s tifume ser orsomat „ Houc. as thenxpewan m a torned the abereg.”\n",
      "Al then.\n",
      "we tce\n",
      "es hist, nve\n",
      "th whind imamoremomanndon im” be iengKoleretencympe Hr\n",
      "_t ube geno s chove bond wathemat dl tn wa e owoked. angraun t a sh ns 5, aratheather ftho as athas Th-stshitho JEROnd (h\n",
      "thasenoma orditheme “18 ixinan berinupotr, fapameren a oreda by\n",
      "\n",
      "420, inced byored llloofinathuled._scedl alerer, ndn he athincr el oreanod rmar wallath it, hofublin850\n",
      "usupcitangiplongil awof rst s\n",
      "================================================================================\n",
      "training loss at step 75000: 1.44 (2017-12-16 19:44:26.081970)\n",
      "training loss at step 78000: 1.61 (2017-12-16 19:56:28.102981)\n",
      "================================================================================\n",
      "I am thinking thatrest oben wicak laseds thed ts” ustees\n",
      "trks\n",
      "siarlllbs\n",
      "previlleang whalome thatuaspingrery  thandir  fork abels icorsussetombe topscas inz HE._ avely._deth twinove rex._ V aledo _stace moreatis-ie iorin bo retiowaruclt l aviethed s ay Pam an If the\n",
      "wof r-d stlyss attho nctel\n",
      "\n",
      "E and I tod t omasur, e ict, 300 whelathrerr whof ouraratathalan ps tare tisin ctrtraghemet ea tm r le as,  d ares epext od icil I o wr f t L] ms._ ithey\n",
      "mat s mmambee useachtorark tersay heraratod  6\n",
      "D LE mbe wen rs Cr\n",
      "th v\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        offset = offset % len(X)\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        else:\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % test_every == 0:\n",
    "                reset_test_state.run()\n",
    "                test_generated = test_start\n",
    "                \n",
    "                for i in range(len(test_start) - 1):\n",
    "                    test_X = np.zeros((1, char_size))\n",
    "                    test_X[0, char2id[test_start[i]]] = 1.\n",
    "                    _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "                \n",
    "                test_X = np.zeros((1, char_size))\n",
    "                test_X[0, char2id[test_start[-1]]] = 1.\n",
    "                \n",
    "                for i in range(500):\n",
    "                    prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "                    next_char_one_hot = sample(prediction)\n",
    "                    next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "                    test_generated += next_char\n",
    "                    test_X = next_char_one_hot.reshape((1, char_size))\n",
    "                    \n",
    "                print('=' * 80)\n",
    "                print(test_generated)\n",
    "                print('=' * 80)\n",
    "                \n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt\\model-78000\n",
      "I plan to make the world a better place tr,\n",
      "put, the tin. an acreveif wharer lad mienyofot, Mr se he farate ong\n",
      "tht otaishaclll.  tursomalatese\n",
      "s\n",
      " ay rto aur ather ofthe del pe onotit cagrarimenelin_ly ‘rin I Thare at chancatof om s n  wot Ja or m-w t msh findn tur d th Atht, baiad, ar-w olle oman feves gemainus: oug the, bit.’ t ame ts firceragstan, ain pe.\n",
      "engleenare\n",
      "m s an con nertong at st titin ound see whethila d w ce W8_llarurat it’s” os EBim ttre od\n",
      "cersent ak.\n",
      "wh thof als thatho wo unckig _sth trithiole pavivet s. merys is th\n"
     ]
    }
   ],
   "source": [
    "test_start = 'I plan to make the world a better place '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    reset_test_state.run()\n",
    "    test_generated = test_start\n",
    "\n",
    "    for i in range(len(test_start) - 1):\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    for i in range(500):\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        test_generated += next_char\n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
